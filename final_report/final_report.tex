\documentclass{article}
\usepackage{fullpage,amsmath,amssymb}
\usepackage{hyperref}
\usepackage[none]{hyphenat}
\usepackage{calc}  % arithmetic in length parameters
\usepackage{caption}
\usepackage{enumitem}  % more control over list formatting
\usepackage{fancyhdr}  % simpler headers and footers
\usepackage{geometry}  % page layout
\usepackage{lastpage}  % for last page number
\usepackage{listings}
\usepackage{parskip}
\usepackage{relsize}  % easier font size changes
\usepackage[normalem]{ulem}  % smarter underlining
\usepackage{url}  % verb-like typesetting of URLs
\usepackage{xcolor}
\usepackage{xfrac}  % nicer looking simple fractions for text and math

\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=false,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4
}
\lstset{style=mystyle}

\everymath{\displaystyle}

\newcommand{\arrayex}[1]{
    \begin{tabular}{|*{20}{c|}}
    \hline
    #1 \\
    \hline
    \end{tabular}
}

\setlength{\tabcolsep}{5pt}

\renewcommand{\arraystretch}{1}

%\usepackage[T1]{fontenc}  % use true 8-bit fonts
%\usepackage{slantsc}  % allow slanted small-caps
%\usepackage{microtype}  % perform various font optimizations
%% Use Palatino-based monospace instead of kpfonts' default.
%\usepackage{newpxtext}

% Common macros.
\input{macros-263}


\geometry{a4paper, margin=1in, headheight=15pt, headsep=20pt}

\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{CSC311 Summer 2024}
\fancyhead[R]{Final Project}
\fancyfoot[C]{\thepage}


\title{Final Report}
\date{\vspace{-7.5ex}}
\hypersetup{pdfpagemode=Fullscreen,
    colorlinks=true,
    linkfileprefix={}}


\begin{document}
\maketitle
\thispagestyle{fancy}

\section*{Part A}

\subsection*{Question 1}

\begin{enumerate}
    \item [(a) (b) (c)] The accuracy on the validation data with $k \in \{1, 6, 11, 16, 21, 26\}$ on user-based and item-based collaborative filtering is as follows:
    \begin{center}
        \includegraphics[width=0.8\textwidth]{q1.png}
    \end{center}
    Test Accuracy on user-based CF with k* = 11: 0.6841659610499576

    Test Accuracy on item-based CF with k* = 21: 0.6816257408975445

    \item [(d)] The test on user-based CF is slightly better than item-based CF.
    
    Additionally, the test accuracy on user-based CF cost less time than item-based CF.

    Therefore, user-based CF is better than item-based CF in this case.

    \item [(e)] 
    \begin{itemize}
        \item The \textsc{knn} algorithm is computational expensive for large datasets.
        \item The Curse of Dimensionality: In high dimensions, ``most'' points are approximately the same distance and the nearest neighbors are not very useful.
    \end{itemize}
\end{enumerate}

\newpage

\subsection*{Question 2}

\begin{enumerate}[label=(\alph*)]
    \item Given the probability that the question $j$ is correctly answered by student $i$ is:
    \begin{equation*}
        p_{ij} =  \frac{\exp(\theta_i - \beta_j)}{1 + \exp(\theta_i - \beta_j)}
    \end{equation*}
    
    The log-likelihood for all students is derived as follows:
    \begin{align*}
        \log p(\mathbf{C}|\boldsymbol{\theta}, \boldsymbol{\beta}) &= \sum_{i, j} (c_{ij} \log p_{ij} + (1 - c_{ij}) \log (1 - p_{ij})) \\
        &= \sum_{i=1}^{n} \sum_{j=1}^{m} \left( c_{ij} \log \left( \frac{\exp(\theta_i - \beta_j)}{1 + \exp(\theta_i - \beta_j)} \right) + (1 - c_{ij}) \log \left( 1 - \frac{\exp(\theta_i - \beta_j)}{1 + \exp(\theta_i - \beta_j)} \right) \right) \\
        &= \sum_{i=1}^{n} \sum_{j=1}^{m} (c_{ij} (\theta_i - \beta_j) - \log(1 + \exp(\theta_i - \beta_j))),
    \end{align*}
    where $c_{ij}$ is the binary response of student $i$ to question $j$.

    The log-likelihood with respect to $\theta_i$ is:
    \begin{align*}
        \frac{\partial \log p(\mathbf{C}|\boldsymbol{\theta}, \boldsymbol{\beta})}{\partial \theta_i} &= \sum_{j=1}^{m} \left( c_{ij} - \frac{\exp(\theta_i - \beta_j)}{1 + \exp(\theta_i - \beta_j)} \right) \\
        &= \sum_{j=1}^{m} (c_{ij} - p_{ij}).
    \end{align*}

    The log-likelihood with respect to $\beta_j$ is:
    \begin{align*}
        \frac{\partial \log p(\mathbf{C}|\boldsymbol{\theta}, \boldsymbol{\beta})}{\partial \beta_j} &= \sum_{i=1}^{n} \left( c_{ij} - \frac{\exp(\theta_i - \beta_j)}{1 + \exp(\theta_i - \beta_j)} \right) \\
        &= \sum_{i=1}^{n} (c_{ij} - p_{ij}).
    \end{align*}

    \item The hyperparameters I selected are: learning rate = 0.01 and iterations = 100.
    
    The training and validation accuracies vs iterations are in the graph below:
    \begin{center}
        \includegraphics[width=0.6\textwidth]{irt_accuracy.png}
    \end{center}

    The log-likelihoods vs iterations are in the graph below:
    \begin{center}
        \includegraphics[width=0.6\textwidth]{irt_llk.png}
    \end{center}

    \item The Final Validation Accuracy: 0.7063223257126728
    
    The Final Test Accuracy: 0.707310189105278

    \item I select the lowest difficulty question $j_1$ (Question 1165), the highest difficulty question $j_2$ (Question 1410) and the average difficulty question $j_3$ (Question 47852).

    The probability of the correct response is in the graph below:
    \begin{center}
        \includegraphics[width=0.6\textwidth]{irt_question.png}
    \end{center}

    The shape of the curves are like the sigmoid function as expected.
    
    Fix a question $j$. As $\theta_i$ increases, the probability of the correct response $p_{ij}$ increases. This means if a student has a higher ability, the probability of the correct response increases.

    Fix a student $i$. As $\beta_j$ increases, the probability of the correct response $p_{ij}$ decreases. This means if a question has a higher difficulty, the probability of the correct response decreases.
\end{enumerate}

\newpage

\subsection*{Question 3}

\textbf{We choose Option 2}
\begin{enumerate}[label=(\alph*)]
    \item 
    \begin{itemize}
        \item ALS breaks down large matrices into lower-dimensional matrices, while neural networks model non-linear relationships through layers.
        
        \item ALS is less flexible than neural networks because it is designed for matrix factorization, whereas neural networks can model non-linear relationships.
        
        \item ALS is more computationally efficient than neural networks for sparse datasets because neural networks require significant computational resources.
    \end{itemize}

    \item The implementation is in \texttt{neural\_network.py}.
    
    \item The optimization hyperparameters we chose are:
    
    \texttt{k = 50, lr = 0.01, num\_epoch = 50}

    The Validation Accuracy we obtained is 0.68981.

    \item The plot with \texttt{k = 50, lr = 0.01, num\_epoch = 50} is shown below:
    
    \includegraphics[width=0.7\linewidth]{6031723149317_.pic.jpg}

    \includegraphics[width=0.7\linewidth]{6041723149328_.pic.jpg}

    The Final Test Accuracy is 0.68558.

    \item The best regularization penalty we found is $\lambda = 0.01$. With this value of $\lambda$, we obtained:
    
    Final Validation Accuracy: 0.67824

    Final Test Accuracy: 0.68078

    The model performed about the same with the regularization penalty. This may be because our model is already well-regularized and does not overfit, or only has negligible overfitting issues.
\end{enumerate}

\newpage

\subsection*{Question 4}

The Final Validation Accuracy is: 0.66286

The Final Test Accuracy is: 0.66949 \\

\textbf{Ensemble process:}

We use three neural network models to implement bagging ensemble. We first randomly sample with replacement from the training dataset. Then we train three different neural networks independently for each training sample. These three neural networks are independent and can run individually. After all models are trained, we use them to make predictions separately, and we take the average of each of their predictions as our final prediction. \\

\textbf{Better or Not:}

No, the bagging model is about the same performance as the single neural network model, so it doesn't improve the performance. \\

\textbf{Reason:}

Ensembling the same model trained on different data subsets lacks model diversity, which does not always improve the model performance.

Additionally, the small training subset could be another problem. When the training set is small, there could be an issue that the training subset is even smaller, so that each model is not well trained, which results in poor performance.

\newpage

\section*{Part B}

\subsection*{Formal Description}

The performance of the IRT algorithm in part A is not satisfactory. We believe that the main reason is that a single decision tree is a high-variance model. We believe that the decision tree in part A overfits the training data, which makes the model's generalization ability poor.

Therefore, we decided to reduce the variance by averaging the predictions of multiple decision trees using a random forest. Each tree is trained on a different random subset of the training data. The results are aggregated at the end to make the model more robust by smoothing the data.

\subsection*{Algorithm Box}

\includegraphics[width=0.7\linewidth]{6051723192250_.pic.jpg}

By training multiple decision trees on different data subsets in the form of random forests, the overfitting problem of the model can be reduced.

\subsection*{Idea Diagram}

\includegraphics[width=1.0\linewidth]{笔记 2024年8月9日.jpeg}

\subsection*{Comparison or Demonstration}

For comparison, when using the single irt algorithm, we obtained the following statistics. We use this group of data as the baseline models:

\includegraphics[width=0.7\linewidth]{6061723195551_.pic.jpg}

When using a random forest consisting of two decision trees, we get the following statistics:

\includegraphics[width=0.7\linewidth]{6081723195750_.pic.jpg}

When using a random forest consisting of three decision trees, we get the following statistics:

\includegraphics[width=0.7\linewidth]{6101723195879_.pic.jpg}

When using a random forest consisting of four decision trees, we get the following statistics:

\includegraphics[width=0.7\linewidth]{6121723196137_.pic.jpg}
    
After comparison, our model does not significantly improve the accuracy.

\subsection*{Experiment to Test Our Hypothesis}

We use the accuracy of the training dataset and the accuracy of the validation dataset to determine whether the model has signs of overfitting. If the training accuracy is significantly higher than the validation accuracy, it means that the model is too sensitive to the training dataset and has signs of overfitting. On the contrary, if the training accuracy and validation accuracy are close, it means that the model does not have overfitting.

The test accuracy of the original model is as follows:

\includegraphics[width=0.7\linewidth]{6071723195572_.pic.jpg}

We can see that the training accuracy of the original model is significantly higher than the validation accuracy, indicating that the original model may be overfitting.

The random forests consisting of 2, 3, and 4 decision trees have the following statistics:

\includegraphics[width=0.7\linewidth]{6091723195756_.pic.jpg}

\includegraphics[width=0.7\linewidth]{6111723195885_.pic.jpg}

\includegraphics[width=0.7\linewidth]{6131723196143_.pic.jpg}

Unfortunately, Random Forest only slightly reduces the gap between training accuracy and validation accuracy, and the overfitting problem still exists.

\subsection*{Limitations}

As mentioned above, implementing Random Forest algorithm did not improve our original model significantly. This could be due to the following reasons:
\begin{itemize}
    \item The given dataset is not large enough or diverse enough. Random Forest is known to perform well on large-scale datasets with many features. In our improved model, we split the dataset into many subsets and trained a Random Forest model on each subset. However, the dataset may not be large enough to benefit from this approach.
    
    Hence, changing the dataset may improve the performance of the Random Forest algorithm.
    
    \item In our Random Forest algorithm, we assumes all predictions from the base models are equally important and we average all predictions of each decision tree to get the final prediction. This may not be the case in practice. Some models may perform better on certain types of data, and simply averaging their predictions may not be the best approach.
    
    We can use a weighted average of the predictions to give more importance to the better performing models, but this requires tuning the weights, which can be time-consuming.

    \item When ensembling multiple models, we need to ensure generalization. It may be because the base models are too similar and the overfitting problem still exists.
    
    We may need to use regularization to limit the complexity of the base models.
   
    \item The Random Forest algorithm is computationally expensive. It requires a lot of time to tune the hyperparameters and train the model. We may not have been able to find the best hyperparameters in the time we had.
   
    \item IRT model itself may be limited in predicting the student's performance. If we integrate more complex models in the ensemble, we may improve the performance of the model.
\end{itemize}

\newpage

\section*{Contributions}

\subsection*{Part A}

\begin{itemize}
    \item Quesion 1 and 2: Mingzhe Zhang
    
    \item Question 3 and 4: Zhiyuan Meng
\end{itemize}

\subsection*{Part B}

\begin{itemize}
    \item Question 1, 2 and 3: Zhiyuan Meng
    
    \item Question 4: Mingzhe Zhang
\end{itemize}

\end{document}
